# -*- coding: utf-8 -*-
"""banks_feature_extraction

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1q4s165j8kSFW1dPzsaN4XrqqSNq5gqLK

Ниже выписаны мои наблюдения для твитов. Ниже произведена попытка их имплементировать.
* повы вклад дает +
* повы ипотек дает -
* повы кредит дает -
* сни ипотек дает +
* сни кредит дает +
* санкц дает -
* города дают 0
* конструкция "как (можно/легко) (взять/узнать/получить/оформить)" дает 0
* http://... - idf высокий. значит надо исключить слова с оригинальной частью ссылки. можно оставить http, оно указывает на 0
* пунктуация, ники пользователей, стоп слова и цифры дают достаточно высокий idf, их исключаем, они не оказывают влияние на тональность

План разбивается на следующие этапы:
1.   Загрузить корпус
2.   Во время загрузки избавиться от ссылок, пунктуации, чисел и ников + стоп-слов
3. Для устранения омонимии провести лемматизацию с помощью spacy
4. Написать функцию, определяющую наличие городов в твите (куда вставить?)
5. Проверить точность выделения признаков с помощью метрики Хи-квадрат

---



---
"""

from google.colab import drive

drive.mount('/content/drive')

from lxml import etree

from typing import List, Tuple

import string, re

# Commented out IPython magic to ensure Python compatibility.
# %pip install spacy-udpipe

import spacy_udpipe

# Commented out IPython magic to ensure Python compatibility.
# %pip install pymorphy2==0.8

spacy_udpipe.download("ru-syntagrus")
nlp = spacy_udpipe.load("ru-syntagrus")

"""

---
Функция для нахождения названий городов
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install flashgeotext

import flashgeotext

import json

# Commented out IPython magic to ensure Python compatibility.
# %pip install toponym

from toponym.recipes import Recipes
from toponym.toponym import Toponym

recipes = Recipes()
recipes.load_from_language(language="russian")

list_of_cities = []
with open('/content/drive/MyDrive/Colab Notebooks/russian-cities.json', 'r', encoding="utf8") as f:
    data = json.load(f)
    for num in range(len(data)):
        list_of_cities.append(data[num]['name'])

dict_of_cities = dict()
for city in list_of_cities:
    t = Toponym(input_word=city, recipes=recipes)
    t.build()
    t_low = Toponym(input_word=city.lower(), recipes=recipes)
    t_low.build()
    both = t_low.list_toponyms() + t.list_toponyms()
    dict_of_cities[city] = both
print(dict_of_cities)

from flashgeotext.geotext import GeoText, GeoTextConfiguration
from flashgeotext.lookup import LookupData

example_text = "http://t.co/YEVHuvVGA1 Взять кредит тюмень альфа банк. Мнение о кредитной карте втб 24 http://t.co/SBJTcsqjCg. «Райффайзенбанк»: Снижение ключевой ставки ЦБ на заседании в Тюмени в эту пятницу очень маловероятно. Современное состояние кредитного поведения в россии сбербанк http://t.co/EXAX01uR0h. @sawik_shuster @YevhenS Главное чтоб банки СБЕР и ВТБ!!!"

lookup_cities = LookupData(
    name="Russian_cities",
    data=dict_of_cities,
    script="cyrillic")

config = GeoTextConfiguration(**{"use_demo_data": False})
geotext = GeoText(config)
geotext.add(lookup_cities)

extract = geotext.extract(example_text)
print(extract)

"""

---

Загрузка и очистка корпуса"""


def rubbish_deleter(s):
    lemmas = []
    for token in nlp(s):
        if token.is_stop:  # проверка на стоп слово
            pass
        else:
            lemmas.append(token.lemma_)  # тут слова лемматизируются с помощью spacy
    s_lemmd = ' '.join(lemmas)
    webs = '((?<=http:\/\/)t\.\S+\.?\S+[\s^\v]?)'
    nums = '(\d+[\s^\v]?)'
    names = '(@\S+)'
    re_webs = re.compile(webs)  # Ссылки
    re_names = re.compile(names)  # Ники
    re_pnct = re.compile('[%s]' % re.escape(string.punctuation))  # Пунктуация
    re_nums = re.compile(nums)  # Числа
    s_dewebbed = re_webs.sub(' ', s_lemmd)
    s_denamed = re_names.sub('', s_dewebbed)
    s_stripped = re_pnct.sub('', s_denamed)
    s_denummed = re_nums.sub('', s_stripped)

    return s_denummed


def load_sentirueval_2016(file_name: str) -> Tuple[List[str], List[str]]:
    texts = []
    labels = []
    with open(file_name, mode='rb') as fp:
        xml_data = fp.read()
    root = etree.fromstring(xml_data)
    for database in root.getchildren():
        if database.tag == 'database':
            for table in database.getchildren():
                if table.tag != 'table':
                    continue
                new_text = None
                new_label = None
                for column in table.getchildren():
                    if column.get('name') == 'text':
                        new_text = rubbish_deleter(column.text)  # дополнено функцией тут
                        if new_label is not None:
                            break
                    elif column.get('name') not in {'id', 'twitid', 'date'}:
                        if new_label is None:
                            label_candidate = str(column.text).strip()
                            if label_candidate in {'0', '1', '-1'}:
                                new_label = 'negative' if label_candidate == '-1' else \
                                    ('positive' if label_candidate == '1' else 'neutral')
                                if new_text is not None:
                                    break
                if (new_text is None) or (new_label is None):
                    raise ValueError('File `{0}` contains some error!'.format(file_name))
                texts.append(new_text)
                labels.append(new_label)
            break
    return texts, labels


fXML_train = '/content/drive/My Drive/Colab Notebooks/bank_train_2016.xml'
fXML_test = '/content/drive/My Drive/Colab Notebooks/banks_test_etalon.xml'

texts, labels = load_sentirueval_2016(fXML)
texts_test, labels_test = load_sentirueval_2016(fXML_test)

print('Number of texts is {0}, number of labels is {1}.'.format(len(texts), len(labels)))

"""

---

Рандомайзер и примеры"""

import random

for idx in random.choices(list(range(len(texts))), k=20):
    print('{0} => {1}'.format(labels[idx], texts[idx]))

positive_tweets = [texts[idx] for idx in range(len(texts)) if labels[idx] == 'positive']
for cur in positive_tweets[:5]: print(cur)
negative_tweets = [texts[idx] for idx in range(len(texts)) if labels[idx] == 'negative']
for cur in negative_tweets[:5]: print(cur)

"""

---

Векторизация через Count"""

from nltk import word_tokenize
from sklearn.feature_extraction.text import CountVectorizer

vectorizer = CountVectorizer(lowercase=True, tokenizer=word_tokenize)  # создаем экземпляр класса CV

import nltk

nltk.download('punkt')

vectorizer.fit(texts)  # вызываем функцию fit() для обучения словам

print(vectorizer.get_feature_names()[0:20])

print(len(vectorizer.get_feature_names()))

X = vectorizer.transform(texts)  # вызываем функцию transform() для кодирования твитов как векторов

print(type(X))  # получаем матрицу для дальнейшего использования

print(texts[0])

print(X[0])  # (номер предложения, номер слова) кол-во слова в предложении (вектор слова?)

print(vectorizer.get_feature_names()[6321])

"""

---
Векторизация через tf-id"""

from sklearn.feature_extraction.text import \
    TfidfTransformer  # используем трансформер на матрице (после countvectorizer), vectorizer сам по себе

transformer = TfidfTransformer().fit(X)

X_transformed = transformer.transform(X)

print(X_transformed[0])  # (предложение, номер слова) тф

print(X_transformed.shape)  # (к-во предложений, к-во слов)

print(vectorizer.get_feature_names()[7199])

# получаем список таплов (слово, его idf)
tokens_with_IDF = list(zip(vectorizer.get_feature_names(), transformer.idf_))

for feature, idf in tokens_with_IDF[0:20]: print('{0:.6f} => {1}'.format(idf, feature))

sorted_tokens_with_IDF = sorted(tokens_with_IDF, key=lambda it: (-it[1], it[0]))

for feature, idf in sorted_tokens_with_IDF[0:20]: print('{0:.6f} => {1}'.format(idf, feature))

"""

---

Разделяем тренировочную выборку на 80-20, занимаемся ее сортировкой"""

from sklearn.feature_selection import SelectPercentile, chi2

selector = SelectPercentile(chi2, percentile=20)

selector.fit(X_transformed, labels)

selected_tokens_with_IDF = [tokens_with_IDF[idx] for idx in selector.get_support(indices=True)]

print(len(selected_tokens_with_IDF))

"""покажем 20 features (признаков) из 20% отобранных методом Хи-квадрат (по алфавиту)"""

for feature, idf in selected_tokens_with_IDF[0:20]: print('{0:.6f} => {1}'.format(idf, feature))

selected_and_sorted_tokens_with_IDF = sorted(selected_tokens_with_IDF, key=lambda it: (-it[1], it[0]))

"""покажем 20 features (признаков) из 20% отобранных методом Хи-квадрат (по idf и по алфавиту) """

for feature, idf in selected_and_sorted_tokens_with_IDF[0:20]: print('{0:.6f} => {1}'.format(idf, feature))

"""покажем 50 наименее важных features (признаков) из 20% отобранных методом Хи-квадрат (по idf и алфавиту)"""

for feature, idf in selected_and_sorted_tokens_with_IDF[-50:]: print('{0:.6f} => {1}'.format(idf, feature))

"""имеет смысл, что ссылки, названия банков и финансовая лексика имеют меньшую значимость, поскольку они часто поовторяются

---
"""